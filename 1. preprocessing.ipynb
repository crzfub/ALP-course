{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cd8258c1-4e65-48f2-9260-e89530f3f193",
      "metadata": {
        "id": "cd8258c1-4e65-48f2-9260-e89530f3f193"
      },
      "source": [
        "# 1. Preprocessing\n",
        "\n",
        "## 1.1 What is Preprocessing?\n",
        "In this notebook, we survey some snippets of code that help 'preprocess' your corpus of texts.\n",
        "\n",
        "The preprocessing of text files is a significant first step in ensuring reliability of all later stylometric analyses. For all texts predating the invention of the printing press, one could argue that this is perhaps even slightly more important than for early modern or modern texts. Stylometry’s application to premodern texts comes with specific desiderata when compared to, for instance, authorship detection of current-day online, electronically available blog posts. As a result of scribal culture, premodern texts can be tremendously varied. These variations can be captured and in some cases they might even be desirable (e.g. when analyzing various recension of a text or comparing writing conventions across scriptoria), but more often than not they are redundant if what you want to analyze is individual writing style.\n",
        "\n",
        "Preprocessing (potentially) entails steps such as these:\n",
        "\n",
        "* the removal and editing of all irrelevant characters in the text: punctuation, numerals, optical character recognition errors, case-folding, titles or annotations, etc …\n",
        "* ‘tokenising’ the text to meaningful units, often word tokens, but others are:\n",
        "  * subwords; not necessarily linguistic yet meaningful word fragments that are picked up as relevant by a language model, e.g. un | believ | able\n",
        "  * morphemes = linguistic meaningful units, e.g. un | believe | able\n",
        "  * syllables (sounds, phonological units), e.g. un | be | liev | a | ble\n",
        "\n",
        "If so desired, preprocessing also takes care of this:\n",
        "* normalization / standardization: align variant orthographical and editorial conventions between text versions\n",
        "* disambiguation, for instance to semantically distinguish homographs (words that are spelled the same) and homonyms (words that sound and/or are spelled the same).\n",
        "* stemming (recover basis stem or morphological root of word tokens\n",
        "* lemmatisation transforms word tokens to a standard dictionary form\n",
        "* PoS(part-of-speech)-tagging and parsing: identify a token’s part of speech and syntactic function\n",
        "* automated scansion for prosodic units of analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9134155-9064-4973-8a0b-f02857ea2495",
      "metadata": {
        "id": "b9134155-9064-4973-8a0b-f02857ea2495"
      },
      "source": [
        "## 1.2 Reading and Handling File Objects\n",
        "\n",
        "The first step is reading in your corpus of texts, so that we can start manipulating them. The steps below will be easier to follow and execute correctly if you have ensured that your file names are formatted as such: ```author-name_text-title.txt```."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab uploads the files into the temporary runtime (not your Drive).\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "zPhq_DsH3Wr9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "3b9bfc4d-7c17-422f-9dfa-19dc9ca372f1"
      },
      "id": "zPhq_DsH3Wr9",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b6845aab-1c3b-424e-84a2-d69971c37fce\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b6845aab-1c3b-424e-84a2-d69971c37fce\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Bernardus-Claraevallensis_Sermones-super-cantica-canticorum.txt to Bernardus-Claraevallensis_Sermones-super-cantica-canticorum (1).txt\n",
            "Saving Petrus-Abaelardus_Commentariorum-super-S-Pauli-Epistolam-ad-Romanos.txt to Petrus-Abaelardus_Commentariorum-super-S-Pauli-Epistolam-ad-Romanos (1).txt\n",
            "Saving Hildegardis-Bingensis_Liber-divinorum-operum.txt to Hildegardis-Bingensis_Liber-divinorum-operum (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c6f147a-692b-4ddf-b0bd-41f9ae119c94",
      "metadata": {
        "id": "1c6f147a-692b-4ddf-b0bd-41f9ae119c94"
      },
      "source": [
        "Below is a rather large chunk of code in which a number of consecutive steps are introduced and combined.\n",
        "First, we declare empty list containers (```authors```, ```titles```, ```texts```), where we will store our metadata and data.\n",
        "We introduce our first stylometric parameter, the sample length (variable ```sample_len```)\n",
        "We then go over all files you have uploaded, and extract the data (texts) and metadata (authors and titles) from the files.\n",
        "\n",
        "In the text itself, we use the ```re``` (RegEx Module in Python, which stands for regular expressions) which removes digits and punctuation from the text (if so desired). We also apply case folding (convert upper to lowercase).\n",
        "\n",
        "Once the data has been 'cleared' of some of the text items we could say are insignificant for stylistic analysis, we proceed by slicing up the data into discrete segments, or chunks of text."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import glob\n",
        "from string import punctuation\n",
        "\n",
        "# Declare empty lists to fill up with our metadata and data\n",
        "authors, titles, texts = [], [], []\n",
        "\n",
        "# We declare some parameters — the 'settings' of our stylometric experiments\n",
        "sample_len = 2000 # word length of text segment\n",
        "\n",
        "# Function to clean and split text\n",
        "def clean_and_split_text(text, sample_len):\n",
        "    words = re.sub(r'[\\d%s]' % re.escape(punctuation), '', text.lower()).split()\n",
        "    return [words[i:i + sample_len] for i in range(0, len(words), sample_len)]\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    author, title = filename.split('/')[-1].split('.')[0].split('_')[:2]\n",
        "    with open(filename, encoding='utf-8-sig') as file:\n",
        "        text = file.read().strip()\n",
        "        bulk = clean_and_split_text(text, sample_len)\n",
        "\n",
        "        for index, sample in enumerate(bulk):\n",
        "            if len(sample) == sample_len:\n",
        "                authors.append(author)\n",
        "                titles.append(f\"{title}_{index + 1}\")\n",
        "                texts.append(\" \".join(sample))\n",
        "\n",
        "# Print summary to confirm things worked\n",
        "print(\"Text processing complete!\")\n",
        "print(f\"Number of text segments: {len(texts)}\")\n",
        "print(f\"Number of authors: {len(set(authors))}\")\n",
        "print(f\"Number of titles/segments: {len(titles)}\")\n",
        "\n",
        "# Optional: print a sample segment\n",
        "print(\"\\nSample processed text segment:\\n\", texts[0][:200], \"...\")\n",
        "print(titles[-5:])"
      ],
      "metadata": {
        "id": "alBqQ4Iv3uSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c11d69ba-4824-492d-dafa-58fae281d5ae"
      },
      "id": "alBqQ4Iv3uSr",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text processing complete!\n",
            "Number of text segments: 185\n",
            "Number of authors: 3\n",
            "Number of titles/segments: 185\n",
            "\n",
            "Sample processed text segment:\n",
            " sancti bernardi abbatis claraevallensis sermones in cantica canticorum sermo i de ipso titulo libri «cantica canticorum salomonis» vobis fratres alia quam aliis de saeculo aut certe aliter dicenda sun ...\n",
            "['Liber-divinorum-operum (1)_57', 'Liber-divinorum-operum (1)_58', 'Liber-divinorum-operum (1)_59', 'Liber-divinorum-operum (1)_60', 'Liber-divinorum-operum (1)_61']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3aa3261-e91a-453a-a06e-be82a3d08d9a",
      "metadata": {
        "id": "f3aa3261-e91a-453a-a06e-be82a3d08d9a"
      },
      "source": [
        "## 1.3 Sampling (Text Segmentation)\n",
        "\n",
        "Despite many permutations, sampling methods generally fall within one of these four categories: (a) discrete, (b) rolling, (c) random and (d) generative.\n",
        "\n",
        "* **Discrete** is as above. A longer text is sliced in discrete pieces according to a predefined fixed sample size, where the next sample picks up the trail where the previous one ended.\n",
        "* **Rolling**: makes use of a sliding window, it 'shingles' your text segments. Rolling segmentation samples the text in non-identical, partially overlapping windows instead of discrete chunks of text. It is generally considered to be a more sensitive way of linearly scanning the stylistic profile of a text, and registers how it changes from first to last word.\n",
        "* **Random**: sentences from a certain author’s entire oeuvre are randomly selected until a predefined sample length limit is reached (e.g. keep on randomly selecting until 1,000 words have been found) in order to come to an almost inexhaustible number of new, real-world representations of the author’s lexical distribution through new combinations.\n",
        "* **Generative**: closely related to random sampling, but takes the idea of inexhaustible representability of a stylistic profile one more step further. Text generation attempts to not only imitate the distribution by making use of extant text samples, but even expands a corpus by generating new text. Needless to say this is an interesting yet underexplored area of research for medieval texts. Some work has been done in this regard for Latin-writing late antique and medieval authors (Manjavacas et al. 2017)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1.3.1 Rolling Sampling\n",
        "\n",
        "The block of code below allows you to apply a relatively easy form of sampling, that of **rolling sampling**. The `step_size`-variable specifies the number of words between the starting indices of consecutive samples. For example, if `step_size=100`, each sample starts 100 words after the previous sample. It determines the amount of overlap between consecutive samples."
      ],
      "metadata": {
        "id": "CLmssxjqKCA-"
      },
      "id": "CLmssxjqKCA-"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "faf30c8a-7bf3-45cb-87fc-a920064a1c21",
      "metadata": {
        "id": "faf30c8a-7bf3-45cb-87fc-a920064a1c21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d263133c-7fc5-4feb-efcc-183a127711a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      author                                           title  \\\n",
            "0  Bernardus-Claraevallensis    Sermones-super-cantica-canticorum (1)-0-1400   \n",
            "1  Bernardus-Claraevallensis  Sermones-super-cantica-canticorum (1)-200-1600   \n",
            "2  Bernardus-Claraevallensis  Sermones-super-cantica-canticorum (1)-400-1800   \n",
            "3  Bernardus-Claraevallensis  Sermones-super-cantica-canticorum (1)-600-2000   \n",
            "4  Bernardus-Claraevallensis  Sermones-super-cantica-canticorum (1)-800-2200   \n",
            "\n",
            "                                                text  \n",
            "0  ﻿sancti bernardi abbatis claraevallensis sermo...  \n",
            "1  militant adversus animam vanus scilicet amor m...  \n",
            "2  joan xiv quae enim societas ei quae desursum e...  \n",
            "3  quae osculum flagitat deinde si se osculari a ...  \n",
            "4  tumultibus praevalent dehinc ne hoc quoque oti...  \n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Process uploaded text files into overlapping word-based samples.\n",
        "Each sample is of fixed length (sample_len), taken in steps of step_size,\n",
        "and stored along with its author and title metadata.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "import pandas as pd\n",
        "\n",
        "# Declare empty lists to fill up with our metadata and data\n",
        "authors = []\n",
        "titles = []\n",
        "texts = []\n",
        "\n",
        "sample_len = 1400 # word length of text segment\n",
        "step_size = 200 # step size \"controls how much you step over\" - will take 1200 + 200 from the next chain.\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    author, title = filename.split(\"/\")[-1].split(\".\")[0].split(\"_\")[:2]\n",
        "    with open(filename, 'r') as file:\n",
        "        text = file.read().lower()\n",
        "        text = re.sub('[%s]' % re.escape(punctuation), '', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        words = text.split()\n",
        "        steps = np.arange(0, len(words), step_size)\n",
        "        for each_begin in steps:\n",
        "            sample_range = range(each_begin, each_begin + sample_len)\n",
        "            sample = [words[index] for index in sample_range if index < len(words)]\n",
        "            if len(sample) == sample_len:\n",
        "                key = '{}-{}-{}'.format(title, str(each_begin), str(each_begin + sample_len))\n",
        "                authors.append(author)\n",
        "                titles.append(key)\n",
        "                texts.append(\" \".join(sample))\n",
        "\n",
        "# Turn results into a dataframe\n",
        "df = pd.DataFrame({\n",
        "    \"author\": authors,\n",
        "    \"title\": titles,\n",
        "    \"text\": texts\n",
        "})\n",
        "\n",
        "# Peek at the data\n",
        "print(df.head())\n",
        "\n",
        "# Count samples per author\n",
        "author_counts = df[\"author\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb10930-fc5e-4f9d-9a74-049b60a17abb",
      "metadata": {
        "id": "cbb10930-fc5e-4f9d-9a74-049b60a17abb"
      },
      "source": [
        "### 1.3.2 Random sampling\n",
        "The code block below gives you a starting point to experiment with **random sampling**. The variable `word_limit` is virtually the same as the desired `sample_len` above: it indicates how many words you want to include in your sample. `n_samples` yields the desired number of random samples per author.\n",
        "\n",
        "\n",
        "#### Idea for Cuneiform\n",
        "**As there are no Sentences maybe try using Words**. I.e. use 7 random words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "dbeb2b44-d5dc-4aef-aeda-58c0bf27c21d",
      "metadata": {
        "id": "dbeb2b44-d5dc-4aef-aeda-58c0bf27c21d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb3a96d-fe5a-4ce1-ecac-55943609498a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      author     title  \\\n",
            "0  Bernardus-Claraevallensis  sample_1   \n",
            "1  Bernardus-Claraevallensis  sample_2   \n",
            "2  Bernardus-Claraevallensis  sample_3   \n",
            "3  Bernardus-Claraevallensis  sample_4   \n",
            "4  Bernardus-Claraevallensis  sample_5   \n",
            "\n",
            "                                                text  \n",
            "0  Non enim ait, saliens in montes; sed, in monti...  \n",
            "1  Arbitror et illud planum, cur se pertransisse ...  \n",
            "2  Sed impius rex recusavit (Isai. Jam oscitat pu...  \n",
            "3  Est et misericors. Porro cui non placet Deus, ...  \n",
            "4  Verum id Pater. Et propterea, quaeso, indica m...  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from string import punctuation\n",
        "import random\n",
        "\n",
        "# Upper word limit for each sample (in words)\n",
        "word_limit = 1400\n",
        "n_samples = 10  # number of randomly generated segments per author\n",
        "\n",
        "# Count the total number of words across a list of sentences\n",
        "def count_words(sentences):\n",
        "    return sum(len(sentence.split()) for sentence in sentences)\n",
        "\n",
        "# Randomly select sentences until the word limit is reached\n",
        "def sample_sentences(sentences, word_limit):\n",
        "    sampled_sentences = []\n",
        "    total_words = 0\n",
        "    remaining_sentences = sentences.copy()\n",
        "\n",
        "    while total_words < word_limit and remaining_sentences:\n",
        "        sentence = random.choice(remaining_sentences)\n",
        "        sentence_word_count = len(sentence.split())\n",
        "        if total_words + sentence_word_count <= word_limit:\n",
        "            sampled_sentences.append(sentence)\n",
        "            total_words += sentence_word_count\n",
        "        remaining_sentences.remove(sentence)\n",
        "\n",
        "    return sampled_sentences\n",
        "\n",
        "data = {}\n",
        "# Read all uploaded files and split them into sentences\n",
        "for filename in uploaded.keys():\n",
        "    author, title = filename.split('/')[-1].split('.')[0].split('_')[:2]\n",
        "    data[author] = []\n",
        "    with open(filename, encoding='utf-8-sig') as file:\n",
        "        text = file.read().strip()\n",
        "        # Split sentences on . or ? while avoiding common abbreviation issues\n",
        "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
        "        for sentence in sentences:\n",
        "            data[author].append(sentence)\n",
        "\n",
        "# Generate random samples per author and collect metadata\n",
        "authors, titles, texts = [], [], []\n",
        "sampled_data = {}\n",
        "for author in data.keys():\n",
        "    sampled_data[author] = []\n",
        "for author, sentences in data.items():\n",
        "    for i in range(0, n_samples):\n",
        "        random_sample = sample_sentences(sentences, word_limit)\n",
        "        random_sample = ' '.join(random_sample)\n",
        "        title = 'sample_' + str(i+1)  # label each random sample\n",
        "        authors.append(author)\n",
        "        titles.append(title)\n",
        "        texts.append(random_sample)\n",
        "\n",
        "# Store results in a dataframe for inspection and analysis\n",
        "df = pd.DataFrame({\n",
        "    \"author\": authors,\n",
        "    \"title\": titles,\n",
        "    \"text\": texts\n",
        "})\n",
        "\n",
        "# Display a few samples (results will change each run due to random selection)\n",
        "print(df.head())\n",
        "\n",
        "# Quick overview: number of samples generated per author\n",
        "author_counts = df[\"author\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xK0n2xS_Rjji"
      },
      "id": "xK0n2xS_Rjji",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}